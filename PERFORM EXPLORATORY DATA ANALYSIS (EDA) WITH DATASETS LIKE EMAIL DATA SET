# Step 1: Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer

# Step 2: Load the dataset
# Downloaded from Kaggle: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset
# The CSV has extra unnamed columns, so we'll drop them.
df = pd.read_csv("https://raw.githubusercontent.com/justmarkham/DAT8/master/data/sms.tsv",
                 sep="\t", names=["v1", "v2"])

# Step 3: Understand the dataset structure
print("First few rows of the dataset:")
print(df.head())

print("\nDataset information (columns, types, and non-null counts):")
print(df.info())

print("\nStatistical summary of the dataset:")
print(df.describe())

# Step 4: Check for null/missing values
print("\nMissing values in each column:")
print(df.isnull().sum())

# Step 5: Analyze class distribution (spam vs ham)
plt.figure(figsize=(8, 6))
sns.countplot(x='v1', data=df, hue='v1', palette='viridis', legend=False)
plt.title('Class Distribution (Ham vs Spam)')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

# Step 6: Visualize data distribution using plots
df['message_length'] = df['v2'].apply(len)

plt.figure(figsize=(10, 6))
sns.histplot(df['message_length'], bins=50, kde=True, color='blue')
plt.title('Distribution of Message Lengths')
plt.xlabel('Message Length')
plt.ylabel('Frequency')
plt.show()

# Step 7: Perform text-specific analysis
# 7a. Message length statistics
average_length = df['message_length'].mean()
median_length = df['message_length'].median()
std_length = df['message_length'].std()

print(f"\nAverage message length: {average_length}")
print(f"Median message length: {median_length}")
print(f"Standard deviation of message length: {std_length}")

# 7b. Word frequency analysis (word clouds)
spam_messages = df[df['v1'] == 'spam']['v2']
ham_messages = df[df['v1'] == 'ham']['v2']

# Spam WordCloud
spam_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(spam_messages))
plt.figure(figsize=(10, 6))
plt.imshow(spam_wordcloud, interpolation='bilinear')
plt.title("Word Cloud for Spam Messages")
plt.axis('off')
plt.show()

# Ham WordCloud
ham_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(ham_messages))
plt.figure(figsize=(10, 6))
plt.imshow(ham_wordcloud, interpolation='bilinear')
plt.title("Word Cloud for Ham Messages")
plt.axis('off')
plt.show()

# 7c. Top words using CountVectorizer
vectorizer = CountVectorizer(stop_words='english', max_features=10)

# Spam
spam_words = vectorizer.fit_transform(spam_messages).toarray()
spam_word_freq = pd.DataFrame(spam_words, columns=vectorizer.get_feature_names_out()).sum(axis=0).sort_values(ascending=False)

# Ham
ham_words = vectorizer.fit_transform(ham_messages).toarray()
ham_word_freq = pd.DataFrame(ham_words, columns=vectorizer.get_feature_names_out()).sum(axis=0).sort_values(ascending=False)

print("\nTop 10 words in Spam messages:\n", spam_word_freq.head(10))
print("\nTop 10 words in Ham messages:\n", ham_word_freq.head(10))

# Step 8: Correlation check
df['is_spam'] = df['v1'].apply(lambda x: 1 if x == 'spam' else 0)
correlation = df[['message_length', 'is_spam']].corr()
print("\nCorrelation between message length and spam/ham:\n", correlation)
